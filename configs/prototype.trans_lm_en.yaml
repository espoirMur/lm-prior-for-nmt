epochs: 200
batch_tokens: 1000
parallel: False
pin_memory: False

logging:
  log_interval: 100
  checkpoint_interval: 1000
  samples_interval: 500
  emb_inspect_interval: 1000
  module_grad_interval: 500

optim:
  optimizer: ranger
  lr: 0.001
  k: 10
  weight_decay: 0.0
  clip: 1

  scheduler: plateau
  step_size: 1
  patience: 2
  eta_min: 0.00001
  min_lr: 0.0001
  gamma: 0.3
  milestones: [5,15]
  early_stop: 10

losses:
  lm:
    tag: lm
    weight: 1
    perplexity: True

data:
  train_path: ../datasets/mono/prototype/en.train
  val_path:   ../datasets/mono/prototype/en.valid
  subword_path: ../datasets/mt/wmt_ende/en.16000

  seq_len: 250
  sos: True
  vocab_size: 10000
  embeddings:


model:
  mode: LM # LM,  AE
  type: transformer

  emb_trainable: True
  emb_size: 512
  nhid: 1024
  nhead: 8
  nlayers: 4
  dropout: 0.1
  inp_dropout: 0.0
  tie_projections: True
